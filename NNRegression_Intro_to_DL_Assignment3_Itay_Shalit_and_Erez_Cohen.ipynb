{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNRegression - Intro to DL Assignment3 - Itay Shalit and Erez Cohen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFMbvNJyie1uJjpkQrNFep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItayShalit/Intro-to-Deep-Learning/blob/main/NNRegression_Intro_to_DL_Assignment3_Itay_Shalit_and_Erez_Cohen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Itay Shalit 207435199  \n",
        "             Erez Cohen 208848531"
      ],
      "metadata": {
        "id": "Vykc-DUiFEFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyhessian\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from pyhessian import hessian\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.utils.data as data_utils\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from sklearn.datasets import make_regression\n",
        "from google.colab import files\n",
        "from math import pi, acos\n",
        "from sklearn.metrics import mean_squared_error\n"
      ],
      "metadata": {
        "id": "l_Q9ewg3FEup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19540ffc-7be6-4857-98b8-9104df2432b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyhessian in /usr/local/lib/python3.7/dist-packages (0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pyhessian) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyhessian) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pyhessian) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Generation"
      ],
      "metadata": {
        "id": "XTBmcHZBIAus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 10\n",
        "x,y = make_regression(100000, 10, random_state = RANDOM_STATE, noise = 0.1)\n",
        "df = pd.DataFrame(x)\n",
        "df['y'] = y\n",
        "train, test = train_test_split(df, test_size = 0.25, random_state = 10)"
      ],
      "metadata": {
        "id": "v5vezJb3H_gF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2"
      ],
      "metadata": {
        "id": "Vd9oCIUwH4fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = len(train)\n",
        "\n",
        "test_target = torch.tensor(test['y'].values.astype(np.float32))#\n",
        "test_target = torch.unsqueeze(test_target, dim = 1)\n",
        "train_target = torch.tensor(train['y'].values.astype(np.float32))\n",
        "train_target = torch.unsqueeze(train_target, dim = 1)\n",
        "test = torch.tensor(test.drop('y', axis = 1).values.astype(np.float32))\n",
        "train = torch.tensor(train.drop('y', axis = 1).values.astype(np.float32))\n",
        "\n",
        "train_tensor = data_utils.TensorDataset(train, train_target) \n",
        "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "B-befXEzUyAa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        print(loss)\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "def calculate_gradient_l2_norm(model):\n",
        "    mean_l2_norm = 0\n",
        "    grad_num = 0\n",
        "    for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        grad_num += len(p)\n",
        "    for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        mean_l2_norm += (p.grad.data.norm(2).item())*(len(p)/grad_num)\n",
        "    return mean_l2_norm\n",
        "\n",
        "def init_weights_normal(model, init_std):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "      torch.nn.init.normal_(m.weight, mean=0.0, std=init_std)\n",
        "      if m.bias is not None:\n",
        "        torch.nn.init.normal_(m.bias, mean=0.0, std=init_std)\n",
        "\n",
        "class linearRegression(torch.nn.Module):\n",
        "    def __init__(self, depth, inputSize, outputSize, hiddenSize):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(torch.nn.Linear(inputSize, hiddenSize))\n",
        "        for i in range(depth-1):\n",
        "          self.layers.append(torch.nn.Linear(hiddenSize, hiddenSize))\n",
        "        self.layers.append(torch.nn.Linear(hiddenSize, outputSize))\n",
        "\n",
        "    def forward(self, inx):\n",
        "        x = inx\n",
        "        for layer in self.layers:\n",
        "          x = layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "BO1KcYD_O3x5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "HIDDEN_SIZE = 16\n",
        "PLOTTING_PARAM = 100\n",
        "\n",
        "\n",
        "epochs = 2000    \n",
        "mseloss = F.mse_loss\n",
        "lr = 10e-7\n",
        "init_std = 10e-1\n",
        "\n",
        "PLOTTING_PARAM = 200\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "test_cpu = test\n",
        "train_cpu = train\n",
        "test_target_cpu = test_target\n",
        "train_target_cpu = train_target\n",
        "\n",
        "all_loss_values, all_loss_grad_l2_norm, all_min_eigenvalues_of_hessian, all_max_eigenvalues_of_hessian  = [], [], [], []    \n",
        "\n",
        "\n",
        "for depth in [2,3,4]:\n",
        "  torch.cuda.empty_cache()\n",
        "  test = test_cpu.to(device)\n",
        "  train = train_cpu.to(device)\n",
        "  test_target = test_target_cpu.to(device)\n",
        "  train_target = train_target_cpu.to(device)\n",
        "  loss_values, loss_grad_l2_norm, min_eigenvalues_of_hessian, max_eigenvalues_of_hessian  = [], [], [], []    \n",
        "  model = linearRegression(depth, 10, 1, 16)    \n",
        "  model = model.to(device) \n",
        "  init_weights_normal(model, init_std)\n",
        "  for epoch in range(epochs):\n",
        "      loss_values.append(mseloss(test_target, model(test)))\n",
        "      loss_grad_l2_norm.append(calculate_gradient_l2_norm(model))  \n",
        "      model.train()\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
        "      optimizer.zero_grad()\n",
        "      loss = mseloss(train_target, model(train))  \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if epoch%PLOTTING_PARAM == 0:\n",
        "        print(f'epoch {epoch}, loss {loss_values[-1]}, train loss {mseloss(train_target, model(train))}, gradient norm {loss_grad_l2_norm[-1]}')   \n",
        "        hessian_comp = hessian(model, F.mse_loss, data=(train, train_target), cuda=True)\n",
        "        top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues(top_n=15)\n",
        "        min_eigenvalues_of_hessian.append(min(top_eigenvalues))\n",
        "        max_eigenvalues_of_hessian.append(max(top_eigenvalues)) \n",
        "  all_loss_values.append(loss_values) \n",
        "  all_loss_grad_l2_norm.append(loss_grad_l2_norm) \n",
        "  all_min_eigenvalues_of_hessian.append(min_eigenvalues_of_hessian) \n",
        "  all_max_eigenvalues_of_hessian.append(max_eigenvalues_of_hessian)        "
      ],
      "metadata": {
        "id": "fmybEUZJBadV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (18, 25))\n",
        "ax1 = fig.add_subplot(3, 1, 1)\n",
        "ax2 = fig.add_subplot(3, 1, 2)\n",
        "ax3 = fig.add_subplot(3, 1, 3)\n",
        "\n",
        "ax1.plot([i for i in range(len(all_loss_values[0]))], all_loss_values[0], label = 'Depth 2')\n",
        "ax1.plot([i for i in range(len(all_loss_values[1]))], all_loss_values[1], label = 'Depth 3')\n",
        "ax1.plot([i for i in range(len(all_loss_values[2]))], all_loss_values[2], label = 'Depth 4')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('L2 Loss')\n",
        "ax1.set_title('Test Loss through Training')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot([i for i in range(len(all_loss_grad_l2_norm[0]))], all_loss_grad_l2_norm[0], label = 'Depth 2')\n",
        "ax2.plot([i for i in range(len(all_loss_grad_l2_norm[1]))], all_loss_grad_l2_norm[1], label = 'Depth 3')\n",
        "ax2.plot([i for i in range(len(all_loss_grad_l2_norm[2]))], all_loss_grad_l2_norm[2], label = 'Depth 4')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('L2-Norm of Gradient')\n",
        "ax2.set_title('Norm of Gradient through Training')\n",
        "ax2.legend()\n",
        "\n",
        "ax3.plot([i for i in range(len(all_min_eigenvalues_of_hessian[0]))], all_min_eigenvalues_of_hessian[0], label = 'Depth 2 Min Eigenvalue')\n",
        "ax3.plot([i for i in range(len(all_min_eigenvalues_of_hessian[1]))], all_min_eigenvalues_of_hessian[1], label = 'Depth 3 Min Eigenvalue')\n",
        "ax3.plot([i for i in range(len(all_min_eigenvalues_of_hessian[2]))], all_min_eigenvalues_of_hessian[2],label = 'Depth 4 Min Eigenvalue')\n",
        "ax3.plot([i for i in range(len(all_max_eigenvalues_of_hessian[0]))], all_max_eigenvalues_of_hessian[0], label = 'Depth 2 Max Eigenvalue')\n",
        "ax3.plot([i for i in range(len(all_max_eigenvalues_of_hessian[1]))], all_max_eigenvalues_of_hessian[1], label = 'Depth 3 Max Eigenvalue')\n",
        "ax3.plot([i for i in range(len(all_max_eigenvalues_of_hessian[2]))], all_max_eigenvalues_of_hessian[2], label = 'Depth 4 Max Eigenvalue')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_title('Minimal and Maximal Eigenvalues of Hessian through Training')\n",
        "ax3.legend()"
      ],
      "metadata": {
        "id": "d3rA53TwPXhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ],
      "metadata": {
        "id": "3vdfeqoOo8_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df, test_size = 0.25, random_state = 10)\n",
        "\n",
        "x_train = np.array(train.drop('y', axis = 1))\n",
        "x_test = np.array(test.drop('y', axis = 1))\n",
        "y_train = np.array(train['y'])\n",
        "y_test = np.array(test['y'])\n",
        "\n",
        "inputSize = 10\n",
        "lr = 10e-7\n",
        "epochs = 10000"
      ],
      "metadata": {
        "id": "c6GgQiYDtHCZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_loss_values_part3 = []\n",
        "\n",
        "\n",
        "for N in [2,3]:\n",
        "  loss_values = []\n",
        "  w = np.random.normal(0, 10e-1, 10).reshape(-1,1)\n",
        "  for t in range(epochs):  \n",
        "    loss_grad = (np.dot(np.transpose(x_train), np.dot(x_train,w) - y_train.reshape(-1,1)))*(2/(x_train.size))\n",
        "    loss_values.append(mean_squared_error(y_test, np.dot(x_test,w)))\n",
        "    update = np.zeros(w.shape)\n",
        "    u1, s1, v1 = np.linalg.svd(np.dot(w.reshape(-1,1), np.transpose(w.reshape(-1,1))))\n",
        "    u2, s2, v2 = np.linalg.svd(np.dot(np.transpose(w.reshape(-1,1)), w.reshape(-1,1)))\n",
        "    for j in (1, N+1):\n",
        "      s1_exp = np.power(s1, (j-1)/N) \n",
        "      s2_exp = np.power(s2, (N-j)/N)\n",
        "      e1 = np.linalg.multi_dot([u1, np.diag(s1_exp), np.transpose(u1)])\n",
        "      e2 = np.linalg.multi_dot([v2, np.diag(s2_exp), np.transpose(v2)])\n",
        "      update += np.linalg.multi_dot([e1, loss_grad.reshape(-1,1), e2])\n",
        "    w = w - lr*update\n",
        "  all_loss_values_part3.append(loss_values)"
      ],
      "metadata": {
        "id": "uZkTjfwKADG9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (12,7))\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot([i for i in range(len(all_loss_values_part3[0][:2500]))], all_loss_values_part3[0][:2500], label = 'Discrete Updates with Formula N=2')\n",
        "ax.plot([i for i in range(len(all_loss_values_part3[1][:2500]))], all_loss_values_part3[1][:2500], label = 'Discrete Updates with Formula N=3')\n",
        "ax.plot([i for i in range(len(all_loss_values[0]))], all_loss_values[0], label = 'GD with Linear Network N=2')\n",
        "ax.plot([i for i in range(len(all_loss_values[1]))], all_loss_values[1], label = 'GD with Linear Network N=3')\n",
        "ax.legend()\n"
      ],
      "metadata": {
        "id": "HJzzUQKrACyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4"
      ],
      "metadata": {
        "id": "qoy96wsS4R43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 10\n",
        "\n",
        "x,y = make_regression(1000, 10, random_state = RANDOM_STATE, noise = 0.1) #We use a smaller dataset here for computational reasons.\n",
        "df = pd.DataFrame(x)\n",
        "df['y'] = y\n",
        "\n",
        "train, test = train_test_split(df, test_size = 0.25, random_state = 10)\n",
        "batch_size = len(train)\n",
        "test_target = torch.tensor(test['y'].values.astype(np.float32))#\n",
        "test_target = torch.unsqueeze(test_target, dim = 1)\n",
        "train_target = torch.tensor(train['y'].values.astype(np.float32))\n",
        "train_target = torch.unsqueeze(train_target, dim = 1)\n",
        "test = torch.tensor(test.drop('y', axis = 1).values.astype(np.float32))\n",
        "train = torch.tensor(train.drop('y', axis = 1).values.astype(np.float32))\n",
        "\n",
        "train_tensor = data_utils.TensorDataset(train, train_target) \n",
        "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "wQxMt3J6xFLI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TangentKernelNet(torch.nn.Module):\n",
        "    def __init__(self, inputSize, hiddenSize):\n",
        "        super(TangentKernelNet, self).__init__()\n",
        "        self.input_layer = torch.nn.Linear(inputSize, hiddenSize)\n",
        "        torch.nn.init.normal_(self.input_layer.weight, mean = 0.0, std = 1.0)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output_layer = torch.nn.Linear(hiddenSize, 1)\n",
        "        w = np.ones(hiddenSize)\n",
        "        w[:int(hiddenSize*0.5)] = -1.\n",
        "        np.random.shuffle(w)\n",
        "        self.output_layer.weight = nn.Parameter(torch.tensor(w).type(torch.FloatTensor).reshape(1,-1))\n",
        "\n",
        "    def forward(self, inx):\n",
        "        x = self.input_layer(inx)\n",
        "        x = self.relu(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "epochs = 15000   \n",
        "\n",
        "\n",
        "mseloss = F.mse_loss\n",
        "lr = 10e-7\n",
        "init_std = 10e-1\n",
        "\n",
        "PLOTTING_PARAM = 200\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "test_cpu = test\n",
        "train_cpu = train\n",
        "test_target_cpu = test_target\n",
        "train_target_cpu = train_target\n",
        "\n",
        "all_loss_values = []\n",
        "\n",
        "widths = [4, 32, 128, 512]\n",
        "\n",
        "for width in widths:\n",
        "  torch.cuda.empty_cache()\n",
        "  test = test_cpu.to(device)\n",
        "  train = train_cpu.to(device)\n",
        "  test_target = test_target_cpu.to(device)\n",
        "  train_target = train_target_cpu.to(device)\n",
        "  loss_values =  []    \n",
        "  model = TangentKernelNet(10, width)\n",
        "  model = model.to(device) \n",
        "  for epoch in range(epochs):\n",
        "      loss_values.append(mseloss(test_target, model(test)))\n",
        "      model.train()\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
        "      optimizer.zero_grad()\n",
        "      loss = mseloss(train_target, model(train))  \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if epoch%PLOTTING_PARAM == 0:\n",
        "        print(f'epoch {epoch}, loss {loss_values[-1]}')   \n",
        "  all_loss_values.append(loss_values)"
      ],
      "metadata": {
        "id": "0JI22pDhk60b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df, test_size = 0.25, random_state = 10)\n",
        "\n",
        "test_target = np.array(test['y'].values.astype(np.float32))#\n",
        "test_target = np.array(test_target).reshape(-1,1)\n",
        "train_target = np.array(train['y'].values.astype(np.float32))\n",
        "train_target = np.array(train_target).reshape(-1,1)\n",
        "test = np.array(test.drop('y', axis = 1).values.astype(np.float32)).transpose()\n",
        "train = np.array(train.drop('y', axis = 1).values.astype(np.float32)).transpose()\n",
        "\n",
        "np.random.RandomState(seed=RANDOM_STATE)\n",
        "\n",
        "\n",
        "u = np.random.normal(loc=0.0, scale=0.001, size=train.shape[1]).reshape(train.shape[1], 1)\n",
        "lr = 10e-7\n",
        "epochs = 1500\n",
        "\n",
        "gram = np.dot(train.transpose(), train)\n",
        "norms = np.linalg.norm(train, axis = 0 , ord= 2).reshape(-1,1)\n",
        "norm_products = np.dot(norms, norms.transpose())\n",
        "quotient = np.divide(gram, norm_products)\n",
        "quotient[quotient > 1] = 1.0 #Fixing a problem caused by numerical inaccuracies.\n",
        "h_star = np.dot(gram,(pi - np.arccos(quotient)))/(2*pi)\n",
        "\n",
        "analytic_loss_values = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  analytic_loss_values.append(np.linalg.norm(train_target - u, ord = 2))\n",
        "  u_grad = -np.dot(h_star, u - train_target)\n",
        "  u = u + (u_grad*lr)\n",
        "  if i%15 == 0:\n",
        "    print(f\"epoch: {i}, loss: {analytic_loss_values[-1]}\")\n"
      ],
      "metadata": {
        "id": "FeMPFCadrRuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(all_loss_values)):\n",
        "  all_loss_values[i] = torch.tensor(all_loss_values[i]).cpu()\n",
        "fig = plt.figure(figsize = (12,7))\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot([i for i in range(len(all_loss_values[0]))], all_loss_values[0], label = 'Width 4')\n",
        "ax.plot([i for i in range(len(all_loss_values[1]))], all_loss_values[1], label = 'Width 32')\n",
        "ax.plot([i for i in range(len(all_loss_values[2]))], all_loss_values[2], label = 'Width 128')\n",
        "ax.plot([i for i in range(len(all_loss_values[3]))], all_loss_values[3], label = 'Width 512')\n",
        "ax.plot([i for i in range(len(analytic_loss_values))], analytic_loss_values, label = 'Analytic Optimization')\n",
        "\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('MSE Loss')\n",
        "ax.legend()\n"
      ],
      "metadata": {
        "id": "qEvjVMSfE4pJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}